{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7_OLupUPC2U3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author      : Caleb Traxler \n",
    "Description : Machine Learning Homework Number 3 Coding Questions\n",
    "\"\"\"\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# !!! MAKE SURE TO USE LinearSVC.decision_function(X), NOT LinearSVC.predict(X) !!!\n",
    "# (this makes ''continuous-valued'' predictions)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47L2XVzBX6c5"
   },
   "source": [
    "# Problem 3: Twitter Analysis Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9Z8E5YL0CzWe"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- input/output\n",
    "######################################################################\n",
    "\n",
    "def read_vector_file(fname):\n",
    "    \"\"\"\n",
    "    Reads and returns a vector from a file.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        fname  -- string, filename\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        labels -- numpy array of shape (n,)\n",
    "                    n is the number of non-blank lines in the text file\n",
    "    \"\"\"\n",
    "    return np.genfromtxt(fname)\n",
    "\n",
    "\n",
    "def write_label_answer(vec, outfile):\n",
    "    \"\"\"\n",
    "    Writes your label vector to the given file.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n",
    "        outfile -- string, output filename\n",
    "    \"\"\"\n",
    "\n",
    "    # for this project, you should predict 70 labels\n",
    "    if(vec.shape[0] != 70):\n",
    "        print(\"Error - output vector should have 70 rows.\")\n",
    "        print(\"Aborting write.\")\n",
    "        return\n",
    "\n",
    "    np.savetxt(outfile, vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i67aTAmrGGHi"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- feature extraction\n",
    "######################################################################\n",
    "\n",
    "def extract_words(input_string):\n",
    "    \"\"\"\n",
    "    Processes the input_string, separating it into \"words\" based on the presence\n",
    "    of spaces, and separating punctuation marks into their own words.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        input_string -- string of characters\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        words        -- list of lowercase \"words\"\n",
    "    \"\"\"\n",
    "\n",
    "    for c in punctuation :\n",
    "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
    "    return input_string.lower().split()\n",
    "\n",
    "\n",
    "def extract_dictionary(infile):\n",
    "    \"\"\"\n",
    "    Given a filename, reads the text file and builds a dictionary of unique\n",
    "    words/punctuations.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile    -- string, filename\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        word_list -- dictionary, (key, value) pairs are (word, index)\n",
    "    \"\"\"\n",
    "\n",
    "    word_list = {}\n",
    "    idx = 0\n",
    "    with open(infile, 'r') as fid :\n",
    "        # process each line to populate word_list\n",
    "        for input_string in fid:\n",
    "            words = extract_words(input_string)\n",
    "            for word in words:\n",
    "                if word not in word_list:\n",
    "                    word_list[word] = idx\n",
    "                    idx += 1\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def extract_feature_vectors(infile, word_list):\n",
    "    \"\"\"\n",
    "    Produces a bag-of-words representation of a text file specified by the\n",
    "    filename infile based on the dictionary word_list.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile         -- string, filename\n",
    "        word_list      -- dictionary, (key, value) pairs are (word, index)\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        feature_matrix -- numpy array of shape (n,d)\n",
    "                          boolean (0,1) array indicating word presence in a string\n",
    "                            n is the number of non-blank lines in the text file\n",
    "                            d is the number of unique words in the text file\n",
    "    \"\"\"\n",
    "\n",
    "    num_lines = sum(1 for line in open(infile,'r'))\n",
    "    num_words = len(word_list)\n",
    "    feature_matrix = np.zeros((num_lines, num_words))\n",
    "\n",
    "    with open(infile, 'r') as fid :\n",
    "        # process each line to populate feature_matrix\n",
    "        for i, input_string in enumerate(fid):\n",
    "            words = extract_words(input_string)\n",
    "            for word in words:\n",
    "                feature_matrix[i, word_list[word]] = 1.0\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-MvTxQPRGOOf"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# functions -- evaluation\n",
    "######################################################################\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Calculates the performance metric based on the agreement between the\n",
    "    true labels and the predicted labels.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        y_true -- numpy array of shape (n,), known labels\n",
    "        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n",
    "        metric -- string, option used to select the performance measure\n",
    "                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n",
    "                           'sensitivity', 'specificity'\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score  -- float, performance score\n",
    "    \"\"\"\n",
    "    # map continuous-valued predictions to binary labels\n",
    "    y_label = np.sign(y_pred)\n",
    "    y_label[y_label==0] = 1\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1a: compute classifier performance\n",
    "    \n",
    "    if metric == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_label)\n",
    "    elif metric == \"f1-score\":\n",
    "        return f1_score(y_true, y_label)\n",
    "    elif metric == \"auroc\":\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    elif metric == \"precision\":\n",
    "        return precision_score(y_true, y_label)\n",
    "    elif metric == \"sensitivity\":\n",
    "        return recall_score(y_true, y_label)\n",
    "    elif metric == \"specificity\":\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_label).ravel()\n",
    "        return tn / float(tn+fp)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Metric.\")\n",
    "    \n",
    "    pass\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n",
    "    Trains classifier on k-1 folds and tests on the remaining fold.\n",
    "    Calculates the k-fold cross-validation performance metric for classifier\n",
    "    by averaging the performance across folds.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf    -- classifier (instance of LinearSVC)\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- model_selection.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score   -- float, average cross-validation performance across k folds\n",
    "    \"\"\"\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1b: compute average cross-validation performance\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.decision_function(X_test)\n",
    "        score = performance(y_test, y_pred, metric)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "                            \n",
    "    \n",
    "    pass\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_linear(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameter of a linear SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameter that 'maximize' the average k-fold CV performance.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- model_selection.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        C -- float, optimal parameter value for linear SVM\n",
    "    \"\"\"\n",
    "\n",
    "    print('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    best_score = -np.inf\n",
    "    best_C = None\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1c: select optimal hyperparameter using cross-validation\n",
    "    \n",
    "    for C in C_range:\n",
    "        clf = LinearSVC(loss='hinge', random_state = 0, C=C)\n",
    "        score = cv_performance(clf, X, y, kf, metric)\n",
    "        print(f\"C: {C}, {metric}, {score}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "    return best_C\n",
    "            \n",
    "    \n",
    "    pass\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def performance_test(clf, X, y, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Estimates the performance of the classifier.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf          -- classifier (instance of LinearSVC)\n",
    "                          [already fit to data]\n",
    "        X            -- numpy array of shape (n,d), feature vectors of test set\n",
    "                          n = number of examples\n",
    "                          d = number of features\n",
    "        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n",
    "        metric       -- string, option used to select performance measure\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        score        -- float, classifier performance\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2b: return performance on test data under a metric.\n",
    "    \n",
    "    y_pred = clf.decision_function(X)\n",
    "    \n",
    "    score = performance(y, y_pred, metric)\n",
    "    \n",
    "    return score\n",
    "    \n",
    "    pass\n",
    "    ### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zMIQRGpYErVF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1811\n",
      "Linear SVM Hyperparameter Selection based on accuracy:\n",
      "C: 0.001, accuracy, 0.7089285714285715\n",
      "C: 0.01, accuracy, 0.7857142857142857\n",
      "C: 0.1, accuracy, 0.8232142857142858\n",
      "C: 1.0, accuracy, 0.8553571428571429\n",
      "C: 10.0, accuracy, 0.8410714285714285\n",
      "C: 100.0, accuracy, 0.8303571428571427\n",
      "Best C for accuracy: 1.0\n",
      "Linear SVM Hyperparameter Selection based on f1-score:\n",
      "C: 0.001, f1-score, 0.8296684118673647\n",
      "C: 0.01, f1-score, 0.8706029281089144\n",
      "C: 0.1, f1-score, 0.8877170585088769\n",
      "C: 1.0, f1-score, 0.8955839997529085\n",
      "C: 10.0, f1-score, 0.8845909645909644\n",
      "C: 100.0, f1-score, 0.8757878679811746\n",
      "Best C for f1-score: 1.0\n",
      "Linear SVM Hyperparameter Selection based on auroc:\n",
      "C: 0.001, auroc, 0.6853714758342924\n",
      "C: 0.01, auroc, 0.8452170538454162\n",
      "C: 0.1, auroc, 0.8880539772727272\n",
      "C: 1.0, auroc, 0.8940758055235903\n",
      "C: 10.0, auroc, 0.8947513305523589\n",
      "C: 100.0, auroc, 0.9013808676160338\n",
      "Best C for auroc: 100.0\n",
      "Linear SVM Hyperparameter Selection based on precision:\n",
      "C: 0.001, precision, 0.7089285714285715\n",
      "C: 0.01, precision, 0.7755639661129191\n",
      "C: 0.1, precision, 0.8331278101671578\n",
      "C: 1.0, precision, 0.8583024785766789\n",
      "C: 10.0, precision, 0.8726513541403275\n",
      "C: 100.0, precision, 0.8701373820678036\n",
      "Best C for precision: 10.0\n",
      "Linear SVM Hyperparameter Selection based on sensitivity:\n",
      "C: 0.001, sensitivity, 1.0\n",
      "C: 0.01, sensitivity, 0.9898734177215189\n",
      "C: 0.1, sensitivity, 0.9495569620253164\n",
      "C: 1.0, sensitivity, 0.8993354430379746\n",
      "C: 10.0, sensitivity, 0.9018354430379747\n",
      "C: 100.0, sensitivity, 0.909367088607595\n",
      "Best C for sensitivity: 0.001\n",
      "Linear SVM Hyperparameter Selection based on specificity:\n",
      "C: 0.001, specificity, 0.0\n",
      "C: 0.01, specificity, 0.30113636363636365\n",
      "C: 0.1, specificity, 0.5333333333333333\n",
      "C: 1.0, specificity, 0.6814393939393939\n",
      "C: 10.0, specificity, 0.6628787878787878\n",
      "C: 100.0, specificity, 0.6191287878787878\n",
      "Best C for specificity: 1.0\n",
      "Test performance for accuracy: 0.7428571428571429\n",
      "Test performance for f1-score: 0.47058823529411764\n",
      "Test performance for auroc: 0.7453838678328474\n",
      "Test performance for precision: 0.6363636363636364\n",
      "Test performance for sensitivity: 1.0\n",
      "Test performance for specificity: 0.8979591836734694\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# main\n",
    "######################################################################\n",
    "\n",
    "def main() :\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # read the tweets and its labels, change the following two lines to your own path.\n",
    "    ### ========== TODO : START ========== ###\n",
    "    file_path = 'tweets.txt'\n",
    "    label_path = 'labelFs.txt'\n",
    "    ### ========== TODO : END ========== ###\n",
    "    dictionary = extract_dictionary(file_path)\n",
    "    print(len(dictionary))\n",
    "    X = extract_feature_vectors(file_path, dictionary)\n",
    "    y = read_vector_file(label_path)\n",
    "    # split data into training (training + cross-validation) and testing set\n",
    "    X_train, X_test = X[:560], X[560:]\n",
    "    y_train, y_test = y[:560], y[560:]\n",
    "\n",
    "    metric_list = [\"accuracy\", \"f1-score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1b: create stratified folds (5-fold CV)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "\n",
    "    # part 1c: for each metric, select optimal hyperparameter for linear SVM using CV\n",
    "    \n",
    "    best_C_values = {}\n",
    "    for metric in metric_list:\n",
    "        best_C = select_param_linear(X_train, y_train, kf, metric)\n",
    "        best_C_values[metric] = best_C\n",
    "        print(f\"Best C for {metric}: {best_C}\")\n",
    "    \n",
    "    # part 2a: train linear SVMs with selected hyperparameters\n",
    "    performances = {}\n",
    "    for metric, best_C in best_C_values.items():\n",
    "        clf = LinearSVC(loss='hinge', random_state=0, C=best_C)\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    # part 2b: test the performance of your classifiers.\n",
    "        y_pred = clf.decision_function(X_test)\n",
    "        performance_score = performance(y_test, y_pred, metric)\n",
    "        performances[metric] = performance_score\n",
    "        print(f\"Test performance for {metric}: {performance_score}\")\n",
    "    \n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W-_mjX0JMes"
   },
   "source": [
    "# Problem 4: Boosting vs. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0uzCdPTkOQSY"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DVxef2sxOmVI"
   },
   "outputs": [],
   "source": [
    "class Data :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        \"\"\"\n",
    "        Data class.\n",
    "        \n",
    "        Attributes\n",
    "        --------------------\n",
    "            X -- numpy array of shape (n,d), features\n",
    "            y -- numpy array of shape (n,), targets\n",
    "        \"\"\"\n",
    "                \n",
    "        # n = number of examples, d = dimensionality\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.Xnames = None\n",
    "        self.yname = None\n",
    "    \n",
    "    def load(self, filename, header=0, predict_col=-1) :\n",
    "        \"\"\"Load csv file into X array of features and y array of labels.\"\"\"\n",
    "        \n",
    "        # determine filename\n",
    "        f = filename\n",
    "        \n",
    "        # load data\n",
    "        with open(f, 'r') as fid :\n",
    "            data = np.loadtxt(fid, delimiter=\",\", skiprows=header)\n",
    "        \n",
    "        # separate features and labels\n",
    "        if predict_col is None :\n",
    "            self.X = data[:,:]\n",
    "            self.y = None\n",
    "        else :\n",
    "            if data.ndim > 1 :\n",
    "                self.X = np.delete(data, predict_col, axis=1)\n",
    "                self.y = data[:,predict_col]\n",
    "            else :\n",
    "                self.X = None\n",
    "                self.y = data[:]\n",
    "        \n",
    "        # load feature and label names\n",
    "        if header != 0:\n",
    "            with open(f, 'r') as fid :\n",
    "                header = fid.readline().rstrip().split(\",\")\n",
    "                \n",
    "            if predict_col is None :\n",
    "                self.Xnames = header[:]\n",
    "                self.yname = None\n",
    "            else :\n",
    "                if len(header) > 1 :\n",
    "                    self.Xnames = np.delete(header, predict_col)\n",
    "                    self.yname = header[predict_col]\n",
    "                else :\n",
    "                    self.Xnames = None\n",
    "                    self.yname = header[0]\n",
    "        else:\n",
    "            self.Xnames = None\n",
    "            self.yname = None\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def load_data(filename, header=0, predict_col=-1) :\n",
    "    \"\"\"Load csv file into Data class.\"\"\"\n",
    "    data = Data()\n",
    "    data.load(filename, header=header, predict_col=predict_col)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_Zcf4WVqJSpe"
   },
   "outputs": [],
   "source": [
    "# Change the path to your own data directory\n",
    "### ========== TODO : START ========== ###\n",
    "titanic = load_data(\"titanic_train.csv\", header=1, predict_col=0)\n",
    "### ========== TODO : END ========== ###\n",
    "X = titanic.X; Xnames = titanic.Xnames\n",
    "y = titanic.y; yname = titanic.yname\n",
    "n,d = X.shape  # n = number of examples, d =  number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3Ta7XHRWQGNo"
   },
   "outputs": [],
   "source": [
    "def error(clf, X, y, ntrials=100, test_size=0.2) :\n",
    "    \"\"\"\n",
    "    Computes the classifier error over a random split of the data,\n",
    "    averaged over ntrials runs.\n",
    "\n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf         -- classifier\n",
    "        X           -- numpy array of shape (n,d), features values\n",
    "        y           -- numpy array of shape (n,), target classes\n",
    "        ntrials     -- integer, number of trials\n",
    "        test_size   -- proportion of data used for evaluation\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "        train_error -- float, training error\n",
    "        test_error  -- float, test error\n",
    "    \"\"\"\n",
    "\n",
    "    train_error = 0\n",
    "    test_error = 0\n",
    "\n",
    "    train_scores = []; test_scores = [];\n",
    "    for i in range(ntrials):\n",
    "        xtrain, xtest, ytrain, ytest = train_test_split (X,y, test_size = test_size, random_state = i)\n",
    "        clf.fit (xtrain, ytrain)\n",
    "\n",
    "        ypred = clf.predict (xtrain)\n",
    "        err = 1 - metrics.accuracy_score (ytrain, ypred, normalize = True)\n",
    "        train_scores.append (err)\n",
    "\n",
    "        ypred = clf.predict (xtest)\n",
    "        err = 1 - metrics.accuracy_score (ytest, ypred, normalize = True)\n",
    "        test_scores.append (err)\n",
    "\n",
    "    train_error =  np.mean (train_scores)\n",
    "    test_error = np.mean (test_scores)\n",
    "    return train_error, test_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "W8-U3un5PjGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using Decision Tree...\n",
      "Training error: 0.014044943820224698\n",
      "Average Test Error (over 100 trials): 0.24104895104895108\n"
     ]
    }
   ],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(a): Implement the decision tree classifier and report the training error.\n",
    "print('Classifying using Decision Tree...')\n",
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict(X)\n",
    "train_error = 1 - metrics.accuracy_score(y, y_pred)\n",
    "print(f\"Training error: {train_error}\")\n",
    "\n",
    "train_error, test_error = error(clf, X, y)\n",
    "print(f\"Average Test Error (over {100} trials): {test_error}\")\n",
    "### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_x_PevK8Q4dx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using Random Forest...\n",
      "Max samples: 71 (Percentage: 0.1), Training Error: 0.1357293497363796, Test Error: 0.19587412587412587\n",
      "Max samples: 142 (Percentage: 0.2), Training Error: 0.10314586994727591, Test Error: 0.18797202797202794\n",
      "Max samples: 213 (Percentage: 0.3), Training Error: 0.0818629173989455, Test Error: 0.18888111888111891\n",
      "Max samples: 284 (Percentage: 0.4), Training Error: 0.05869947275922671, Test Error: 0.19216783216783218\n",
      "Max samples: 356 (Percentage: 0.5), Training Error: 0.03388400702987697, Test Error: 0.19888111888111892\n",
      "Max samples: 427 (Percentage: 0.6), Training Error: 0.017785588752196824, Test Error: 0.20111888111888113\n",
      "Max samples: 498 (Percentage: 0.7), Training Error: 0.012390158172232001, Test Error: 0.20475524475524473\n",
      "Max samples: 569 (Percentage: 0.8), Training Error: 0.011528998242530775, Test Error: 0.20671328671328676\n",
      "Best setting: max_samples = 142, with Test Error = 0.18797202797202794\n"
     ]
    }
   ],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(b): Implement the random forest classifier and adjust the number of samples used in bootstrap sampling.\n",
    "print('Classifying using Random Forest...')\n",
    "n = len(X)\n",
    "sample_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "best_test_error = float('inf')\n",
    "best_max_samples = None\n",
    "\n",
    "for percentage in sample_percentages:\n",
    "    max_samples = int(n * percentage)\n",
    "    \n",
    "    clf = RandomForestClassifier(criterion='entropy', random_state=0, max_samples=max_samples)\n",
    "    train_error, test_error = error(clf, X, y)\n",
    "    \n",
    "    print(f\"Max samples: {max_samples} (Percentage: {percentage}), Training Error: {train_error}, Test Error: {test_error}\")\n",
    "    \n",
    "    if test_error < best_test_error:\n",
    "          best_test_error = test_error\n",
    "          best_max_samples = max_samples \n",
    "print(f\"Best setting: max_samples = {best_max_samples}, with Test Error = {best_test_error}\")\n",
    "### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZFUyPTPwT53v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying using Random Forest...\n",
      "Max features: 1, Training Error: 0.10121265377855888, Test Error: 0.18776223776223777\n",
      "Max features: 2, Training Error: 0.10314586994727591, Test Error: 0.18797202797202794\n",
      "Max features: 3, Training Error: 0.10244288224956065, Test Error: 0.1872727272727273\n",
      "Max features: 4, Training Error: 0.10430579964850617, Test Error: 0.1874125874125874\n",
      "Max features: 5, Training Error: 0.10544815465729351, Test Error: 0.1886013986013986\n",
      "Max features: 6, Training Error: 0.10581722319859402, Test Error: 0.189020979020979\n",
      "Max features: 7, Training Error: 0.10776801405975397, Test Error: 0.18895104895104897\n",
      "Max features: 8, Training Error: 0.10776801405975397, Test Error: 0.18895104895104897\n",
      "Best setting: max_features = 3, with Test Error = 0.1872727272727273\n"
     ]
    }
   ],
   "source": [
    "### ========== TODO : START ========== ###\n",
    "# Part 4(c): Implement the random forest classifier and adjust the number of features for each decision tree.\n",
    "print('Classifying using Random Forest...')\n",
    "\n",
    "best_max_samples = 142\n",
    "\n",
    "best_test_error = float('inf')\n",
    "best_max_features = None\n",
    "\n",
    "for max_features in range(1, 9):\n",
    "    clf = RandomForestClassifier(criterion='entropy', random_state=0, max_samples=best_max_samples, max_features=max_features)\n",
    "    train_error, test_error = error(clf, X,y)\n",
    "    \n",
    "    print(f\"Max features: {max_features}, Training Error: {train_error}, Test Error: {test_error }\")\n",
    "    \n",
    "    if test_error < best_test_error:\n",
    "        best_test_error = test_error\n",
    "        best_max_features = max_features \n",
    "print(f\"Best setting: max_features = {best_max_features}, with Test Error = {best_test_error}\")\n",
    "\n",
    "### ========== TODO : END ========== ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
